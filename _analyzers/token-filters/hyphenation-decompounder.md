---
layout: default
title: Hyphenation decompounder
parent: Token filters
nav_order: 170
---
<!-- vale off -->
# Hyphenation decompounder token filter
<!-- vale on -->
The `hyphenation_decompounder` token filter in OpenSearch is used to break down compound words into their constituent parts. This filter is particularly useful for languages like German, Dutch, and Swedish, where compound words are common. The filter uses hyphenation patterns (typically defined in .xml files) to identify the possible locations within a compound word where it can be split into components. These components are then checked against a provided dictionary. If there is a match, those components are treated as valid tokens.

## Parameters

The `hyphenation_decompounder` token filter in OpenSearch can be configured with the following parameters:

- `hyphenation_patterns_path`: Path (relative to `config` directory or absolute) to the hyphenation patterns file, which contains the language-specific rules for where to split words. The file is typically in XML format. Sample files can be downloaded from [OFFO Sourceforge project](https://sourceforge.net/projects/offo/). (String, _Required_)
- `word_list`: A list of words that are used to validate the components generated by the hyphenation patterns. (Array of strings, _Required_ if `word_list_path` is not set)
- `word_list_path`: Path (relative to `config` directory or absolute) to list of subwords. (String, _Required_ if `word_list` is not set)
- `max_subword_size`: Maximum subword length. If the generated subword is longer, it will not be added to the generated tokens. Default is `15` (Integer, _Optional_)
- `min_subword_size`: Minimum subword length. If the generated subword is shorted, it will not be added to the generated tokens. Default is `2` (Integer, _Optional_)
- `min_word_size`: Minimum word character length. Shorter word tokens are excluded from decomposition into subwords. Default is `5` (Integer, _Optional_)
- `only_longest_match`: Only include the longest subword in the generated tokens. Default os `false`. (Boolean, _Optional_) 

## Example

The following example request creates a new index named `test_index` and configures an analyzer with `hyphenation_decompounder` filter:

```json
PUT /test_index
{
  "settings": {
    "analysis": {
      "filter": {
        "my_hyphenation_decompounder": {
          "type": "hyphenation_decompounder",
          "hyphenation_patterns_path": "analysis/hyphenation_patterns.xml",
          "word_list": ["notebook", "note", "book"],
          "min_subword_size": 3,
          "min_word_size": 5,
          "only_longest_match": false
        }
      },
      "analyzer": {
        "my_analyzer": {
          "type": "custom",
          "tokenizer": "standard",
          "filter": [
            "lowercase",
            "my_hyphenation_decompounder"
          ]
        }
      }
    }
  }
}
```
{% include copy-curl.html %}

## Generated tokens

Use the following request to examine the tokens generated using the created analyzer:

```json
POST /test_index/_analyze
{
  "analyzer": "my_analyzer",
  "text": "notebook"
}
```
{% include copy-curl.html %}

The response contains the generated tokens:

```json
{
  "tokens": [
    {
      "token": "notebook",
      "start_offset": 0,
      "end_offset": 8,
      "type": "<ALPHANUM>",
      "position": 0
    },
    {
      "token": "note",
      "start_offset": 0,
      "end_offset": 8,
      "type": "<ALPHANUM>",
      "position": 0
    },
    {
      "token": "book",
      "start_offset": 0,
      "end_offset": 8,
      "type": "<ALPHANUM>",
      "position": 0
    }
  ]
}
```
